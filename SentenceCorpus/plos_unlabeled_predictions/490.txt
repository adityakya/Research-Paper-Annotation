MISC--: fundamental problem neuroscience understanding how working memory ability store information at intermediate timescales like tens seconds implemented realistic neuronal networks
MISC--: most likely candidate mechanism attractor network great deal effort gone toward investigating theoretically
MISC--: yet despite almost quarter century intense work attractor networks not fully understood
MISC--: particular there still two unanswered questions
OWNX--: first how attractor networks exhibit irregular firing observed experimentally during working memory tasks
MISC--: second how many memories stored under biologically realistic conditions
AIMX--: here we answer both questions studying attractor neural network inhibition excitation balance each other
AIMX--: using mean field analysis we derive three variable description attractor networks
MISC--: this description follows irregular firing exist only if number neurons involved memory large
MISC--: same mean field analysis also shows number memories stored network scales number excitatory connections result been suggested simple models but never shown realistic ones
MISC--: both predictions verified using simulations large networks spiking neurons
MISC--: critical component any cognitive system working memory mechanism storing information about past events accessing information at later times
OWNX--: without mechanism even simple tasks deciding whether wear heavy jacket light sweater after hearing weather report would impossible
MISC--: although not known exactly how storage retrieval information implemented neural systems very natural way through attractor networks
MISC--: networks transient events world trigger stable patterns activity brain so looking at pattern activity at current time other areas brain know something about what happened past
MISC--: there now considerable experimental evidence attractor networks areas inferior temporal cortex citation citation prefrontal cortex citation citation hippocampus citation citation
MISC--: theoretical standpoint well understood how attractor networks could implemented neuronal networks at least principle
OWNX--: essentially all needed increase connection strength among subpopulations neurons
MISC--: if increase sufficiently large then each subpopulation active without input thus remember events happened past
MISC--: while basic theory attractor networks been known some time citation citation moving past principle qualifier understanding how attractors could implemented realistic spiking networks been difficult
OWNX--: this because original hopfield model violated several important principles neurons did not obey dale s law when memory was activated neurons fired near saturation much higher than observed experimentally working memory tasks citation citation there was no null background state no state all neurons fired at low rates
MISC--: most problems been solved
MISC--: first dale s law was violated was solved clipping synaptic weights using hopfield prescription citation assigning neurons either excitatory inhibitory then setting any weights wrong sign zero citation citation
MISC--: second building hopfield type network low firing rate was solved adding appropriate inhibition citation citation
MISC--: third problem no null background was solved either making units sufficiently stochastic citation citation adding external input citation citation citation
MISC--: spite advancements there still two fundamental open questions
MISC--: one how we understand highly irregular firing observed experimentally working memory tasks citation answering this question important because irregular firing thought play critical role both how fast computations carried out citation ability networks perform statistical inference citation
MISC--: answering hard though because pointed out citation naive scaling net synaptic drive foreground neurons proportional number connections per neuron
OWNX--: consequently because high connectivity observed cortex mean synaptic drive much larger than fluctuations implies foreground neurons should fire regularly
MISC--: moreover pointed out renart et al citation even models move beyond naive scaling produce irregularly firing neurons foreground neurons still tend fire more regularly than background neurons something inconsistent experiments citation
MISC--: several studies attempted get around this problem either directly indirectly citation citation citation
CONT--: most them however did not investigate scaling network parameters its size
MISC--: so although parameters were found led irregular activity was not clear how those parameters should scale size network increased realistic values
OWNX--: two did investigate scaling citation citation irregular firing was possible only if small fraction neurons was involved each memory i e only if coding level was very small
MISC--: although there been no direct measurements coding level during persistent activity at least our knowledge experiments superior temporal sulcus citation suggest much larger than one used models
OWNX--: we should point out though model renart et al citation only one foreground neurons at least regular background neurons
MISC--: second open question what storage capacity realistic attractor networks
MISC--: how many different memories stored single network
OWNX--: answering this critical understanding highly flexible seemingly unbounded memory capacity observed animals
MISC--: simple albeit unrealistic models answer known shown seminal work amit gutfreund sompolinsky citation number memories stored classical hopfield network citation about times number neurons
MISC--: slightly more realistic networks answer also known citation citation citation citation citation citation
MISC--: however even more realistic studies lacked biological plausibility at least one way connectivity was all all rather than sparse citation citation citation citation neurons were binary citation citation citation citation citation citation there was no null background citation citation citation citation citation citation firing rate foreground state was higher than observed experimentally citation citation citation citation citation citation coding level was very small citation citation
AIMX--: here we answer both questions we show realistic networks spiking neurons how irregular firing achieved we compute storage capacity
CONT--: our analysis uses relatively standard mean field techniques requires only one assumption neurons network fire asynchronously
OWNX--: given this assumption we first show neurons fire irregularly only if coding level above some threshold although feature our model foreground neurons slightly more regular than background neurons
AIMX--: we then show maximum number memories our network capacity proportional number connections per neuron result consistent simplified models discussed above
MISC--: predictions verified simulations biologically plausible networks spiking neurons
