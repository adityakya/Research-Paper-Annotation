OWNX--: we consider regularized support vector machines svms show they precisely equivalent new robust optimization formulation
AIMX--: we show this equivalence robust optimization regularization implications both algorithms analysis
MISC--: terms algorithms equivalence suggests more general svm like algorithms classification explicitly build protection noise at same time control overfitting
MISC--: analysis front equivalence robustness regularization provides robust optimization interpretation success regularized svms
OWNX--: we use this new robustness interpretation svms give new proof consistency kernelized svms thus establishing robustness reason regularized svms generalize well
MISC--: support vector machines svms short originated citation traced back early citation citation
MISC--: they continue one most successful algorithms classification
MISC--: svms address classification problem finding hyperplane feature space achieves maximum sample margin when training samples separable leads minimizing norm classifier
MISC--: when samples not separable penalty term approximates total training error considered citation
MISC--: well known minimizing training error itself lead poor classification performance new unlabeled data approach may poor generalization error because essentially overfitting citation
MISC--: variety modifications been proposed combat this problem one most popular methods being minimizing combination training error regularization term
MISC--: latter typically chosen norm classifier
MISC--: resulting regularized classifier performs better new data
MISC--: this phenomenon often interpreted statistical learning theory view regularization term restricts complexity classifier hence deviation testing error training error controlled citation
OWNX--: this paper we consider different setup assuming training data generated true underlying distribution but some non iid potentially adversarial disturbance then added samples we observe
OWNX--: we follow robust optimization citation approach i e minimizing worst possible empirical error under disturbances
MISC--: use robust optimization classification not new citation
MISC--: robust classification models studied past considered only box type uncertainty sets allow possibility data all been skewed some non neutral manner correlated disturbance
CONT--: this made difficult obtain non conservative generalization bounds
MISC--: moreover there not been explicit connection regularized classifier although at high level known regularization robust optimization related citation
OWNX--: main contribution this paper solving robust classification problem class non box typed uncertainty sets providing linkage between robust classification standard regularization scheme svms
OWNX--: particular our contributions include following we solve robust svm formulation class non box type uncertainty sets
OWNX--: this permits finer control adversarial disturbance restricting satisfy aggregate constraints across data points therefore reducing possibility highly correlated disturbance
OWNX--: we show standard regularized svm classifier special case our robust classification thus explicitly relating robustness regularization
MISC--: this provides alternative explanation success regularization also suggests new physically motivated ways construct regularization terms
OWNX--: we relate our robust formulation several probabilistic formulations
OWNX--: we consider chance constrained classifier i e classifier probabilistic constraints misclassification show our robust formulation approximate far less conservatively than previous robust formulations could possibly do
OWNX--: we also consider bayesian setup show this used provide principled means selecting regularization coefficient without cross validation
OWNX--: we show robustness perspective stemming non iid
MISC--: analysis useful standard learning iid setup using prove consistency standard svm classification without using vc dimension stability arguments
AIMX--: this result implies generalization ability direct result robustness local disturbances therefore suggests new justification good performance consequently allows us construct learning algorithms generalize well robustifying non consistent algorithms subsubsection robustness regularization we comment here explicit equivalence robustness regularization
AIMX--: we briefly explain how this observation different previous work why interesting
MISC--: certain equivalence relationships between robustness regularization been established problems other than classification citation but their results do not directly apply classification problem
MISC--: indeed research classifier regularization mainly discusses its effect bounding complexity function class citation
MISC--: meanwhile research robust classification not attempted relate robustness regularization citation part due robustness formulations used those papers
MISC--: fact they all consider robustified versions regularized classifications
MISC--: citation considers robust formulation box type uncertainty relates this robust formulation regularized svm
CONT--: however this formulation involves non standard loss function does not bound symbol loss hence its physical interpretation not clear
OWNX--: connection robustness regularization svm context important following reasons
OWNX--: first gives alternative potentially powerful explanation generalization ability regularization term
MISC--: classical machine learning literature regularization term bounds complexity class classifiers
MISC--: robust view regularization regards testing samples perturbed copy training samples
OWNX--: we show when total perturbation given bounded regularization term bounds gap between classification errors svm two sets samples
OWNX--: contrast standard pac approach this bound depends neither how rich class candidate classifiers nor assumption all samples picked iid
MISC--: manner
MISC--: addition this suggests novel approaches designing good classification algorithms particular designing regularization term
OWNX--: pac structural risk minimization approach regularization chosen minimize bound generalization error based training error complexity term
MISC--: this complexity term typically leads overly emphasizing regularizer indeed this approach known often too pessimistic citation problems more structure
OWNX--: robust approach offers another avenue
MISC--: since both noise robustness physical processes close investigation application noise characteristics at hand provide insights into how properly robustify therefore regularize classifier
MISC--: example known normalizing samples so variance among all features roughly same process commonly used eliminate scaling freedom individual features often leads good generalization performance
OWNX--: robustness perspective this simply says noise anisotropic ellipsoidal rather than spherical hence appropriate robustification must designed fit this anisotropy
OWNX--: we also show using robust optimization viewpoint we obtain some probabilistic results outside pac setup
OWNX--: section we bound probability noisy training sample correctly labeled
MISC--: bound considers behavior corrupted samples hence different known pac bounds
MISC--: this helpful when training samples testing samples drawn different distributions some adversary manipulates samples prevent them being correctly labeled e g spam senders change their patterns time time avoid being labeled filtered
OWNX--: finally this connection robustification regularization also provides us new proof techniques well see section
MISC--: we need point out there several different definitions robustness literature
OWNX--: this paper well aforementioned robust classification papers robustness mainly understood robust optimization perspective where min max optimization performed over all possible disturbances
MISC--: alternative interpretation robustness stems rich literature robust statistics citation studies how estimator algorithm behaves under small perturbation statistics model
MISC--: example influence function approach proposed citation citation measures impact infinitesimal amount contamination original distribution quantity interest
MISC--: based this notion robustness citation showed many kernel classification algorithms including svm robust sense having finite influence function
MISC--: similar result regression algorithms shown citation smooth loss functions citation non smooth loss functions where relaxed version influence function applied
OWNX--: machine learning literature another widely used notion closely related robustness stability where algorithm required robust sense output function does not change significantly under specific perturbation deleting one sample training set
MISC--: now well known stable algorithm svm desirable generalization properties statistically consistent under mild technical conditions see example citation details
OWNX--: one main difference between robust optimization other robustness notions former constructive rather than analytical
OWNX--: contrast robust statistics stability approach measures robustness given algorithm robust optimization robustify algorithm converts given algorithm robust one
AIMX--: example we show this paper ro version naive empirical error minimization well known svm
AIMX--: constructive process ro approach also leads additional flexibility algorithm design especially when nature perturbation known well estimated structure paper this paper organized follows
OWNX--: section we investigate correlated disturbance case show equivalence between robust classification regularization process
OWNX--: we develop connections probabilistic formulations section prove consistency result based robustness analysis section
OWNX--: kernelized version investigated section
OWNX--: some concluding remarks given section notation capital letters used denote matrices boldface letters used denote column vectors
OWNX--: given norm symbol we use symbol denote its dual norm i e symbol
OWNX--: vector symbol positive semi definite matrix symbol same dimension symbol denotes symbol
OWNX--: we use symbol denote disturbance affecting samples
MISC--: we use superscript symbol denote true value uncertain variable so symbol true but unknown noise symbol sample
MISC--: set non negative scalars denoted symbol
MISC--: set integers symbol symbol denoted symbol
