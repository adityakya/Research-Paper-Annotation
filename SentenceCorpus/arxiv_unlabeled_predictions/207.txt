MISC--: large systems important agents learn act effectively but sophisticated multi agent learning algorithms generally do not scale
MISC--: alternative approach find restricted classes games where simple efficient algorithms converge
MISC--: shown stage learning efficiently converges nash equilibria large anonymous games if best reply dynamics converge
MISC--: two features identified improve convergence
MISC--: first rather than making learning more difficult more agents actually beneficial many settings
MISC--: second providing agents statistical information about behavior others significantly reduce number observations needed
MISC--: designers distributed systems frequently unable determine how agent system should behave because optimal behavior depends user s preferences actions others
CONT--: natural approach agents use learning algorithm
MISC--: many multiagent learning algorithms been proposed including simple strategy update procedures fictitious play citation multiagent versions q learning citation no regret algorithms citation
OWNX--: however we discuss section existing algorithms generally unsuitable large distributed systems
MISC--: distributed system each agent limited view actions other agents
MISC--: algorithms require knowing example strategy chosen every agent cannot implemented
MISC--: furthermore size distributed systems requires fast convergence
CONT--: users may use system short periods time conditions system change over time so practical algorithm system thousands millions users needs convergence rate sublinear number agents
MISC--: existing algorithms tend provide performance guarantees polynomial even exponential
MISC--: finally large number agents system guarantees there will noise
MISC--: agents will make mistakes will behave unexpectedly
MISC--: even if no agent changes his strategy there still noise agent payoffs
MISC--: example gossip protocol will match different agents round round congestion underlying network may effect message delays between agents
CONT--: learning algorithm needs robust this noise
CONT--: while finding algorithm satisfies requirements arbitrary games may difficult distributed systems characteristics make problem easier
MISC--: first they involve large number agents
MISC--: having more agents may seem make learning harder after all there more possible interactions
CONT--: however advantage outcome action typically depends only weakly what other agents do
OWNX--: this makes outcomes robust noise
MISC--: having large number agents also make less useful agent try influence others becomes better policy try learn optimal response
MISC--: contrast small number agents agent attempt guide learning agents into outcome beneficial him
MISC--: second distributed systems often anonymous citation does not matter who does something but rather how many agents do
MISC--: example when there congestion link experience single agent does not depend who sending packets but how many being sent
MISC--: finally perhaps most importantly distributed system system designer controls game agents playing
MISC--: this gives us somewhat different perspective than most work takes game given
OWNX--: we do not need solve hard problem finding efficient algorithm all games
MISC--: instead we find algorithms work efficiently interesting classes games where us interesting means type games system designer might wish agents play
MISC--: games should well behaved since would strange design system where agent s decisions influence other agents pathological ways
OWNX--: section we show stage learning citation robust implementable minimal information converges efficiently interesting class games
MISC--: this algorithm agents divide rounds game into series stages
OWNX--: each stage agent uses fixed strategy except he occasionally explores
OWNX--: at end stage agent chooses his strategy next stage whatever strategy had highest average reward current stage
OWNX--: we prove under appropriate conditions large system stage learners will follow approximate best reply dynamics despite errors exploration
OWNX--: games where best reply dynamics converge our theorem guarantees learners will play approximate nash equilibrium
CONT--: contrast previous results where convergence guarantee scales poorly number agents our theorem guarantees convergence finite amount time infinite number agents
MISC--: while assumption best reply dynamics converge strong one many interesting games converge under best reply dynamics including dominance solvable games games monotone best replies
MISC--: marden et al citation observed convergence best reply dynamics often property games humans design
MISC--: moreover convergence best reply dynamics weaker assumption than common assumption made mechanism design literature games interest dominant strategies each agent strategy optimal no matter what other agents do
MISC--: simulation results presented section show convergence fast practice system thousands agents converge few thousand rounds
OWNX--: furthermore we identify two factors determine rate quality convergence
MISC--: one number agents having more agents makes noise systen more consistent so agents learn using fewer observations
MISC--: other giving agents statistical information about behavior other agents this speed convergence order magnitude
MISC--: indeed even noisy statistical information about agent behavior should relatively easy obtain disseminate significantly improve performance
