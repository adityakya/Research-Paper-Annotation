MISC--: fitness functions based test cases very common genetic programming gp
MISC--: this process assimilated learning task inference models limited number samples
AIMX--: this paper investigation two methods improve generalization gp based learning selection best run individuals using three data sets methodology application parsimony pressure order reduce complexity solutions
AIMX--: results using gp binary classification setup show while accuracy test sets preserved less variances compared baseline results mean tree size obtained tested methods significantly reduced
MISC--: gp particularly suited problems assimilated learning tasks minimization error between obtained desired outputs limited number test cases training data using ml terminology
CONT--: indeed classical gp examples symbolic regression boolean multiplexer artificial ant citation only simple instances well known learning problems i e respectively regression binary classification reinforcement learning
OWNX--: early years gp problems were tackled using single data set reporting results same data set was used evaluate fitnesses during evolution
OWNX--: this was justifiable fact toy problems used only illustrate potential gp
MISC--: ml community recognized methodology flawed given learning algorithm overfit data used during training perform poorly unseen data same application domain citation
OWNX--: hence important report results set data was not used during learning stage
OWNX--: this what we call this paper two data sets methodology training set used learning algorithm test set used report performance algorithm unseen data good indicator algorithm s generalization robustness capability
CONT--: even though this methodology been widely accepted applied ml pr communities long time ec community still lags behind publishing papers reporting results data sets were used during evolution training phase
MISC--: this methodological problem already been spotted see citation should less less common future
MISC--: two data sets methodology prevents reporting flawed results learning algorithms overfit training set
MISC--: but this does not prevent itself overfitting training set
MISC--: common approach add third data set validation set helps learning algorithm measure its generalization capability
MISC--: this validation set useful interrupt learning algorithm when overfitting occurs select configuration learning machine maximizes generalization performances
OWNX--: this third data set commonly used train classifiers back propagation neural networks easily applied ec based learning
MISC--: but this approach important drawback removes significant amount data training set harmful learning process
MISC--: indeed richer training set more representative real data distribution more learning algorithm expected converge toward robust solutions
AIMX--: light considerations objective this paper investigate effect validation set select best run individuals gp based learning application
MISC--: another concern ml pr communities develop learning algorithms generate simple solutions
OWNX--: argument behind this occam s razor principle states between solutions comparable quality simplest solutions must preferred
MISC--: another argument minimum description length principle citation states best model one minimizes amount information needed encode model data given model
MISC--: preference simpler solutions overfitting avoidance closely related more likely complex solution incorporates specific information training set thus overfitting training set compared simpler solution
MISC--: but mentioned citation this argumentation should taken care too much emphasis minimizing complexity prevent discovery more complex yet more accurate solutions
MISC--: there strong link between minimization complexity gp based learning control code bloat citation exaggerated growth program size course gp runs
MISC--: even though complexity code bloat not exactly same phenomenon some kind bloat generated neutral pieces code no effect actual complexity solutions most mechanisms proposed control citation also used minimize complexity solutions obtained gp based learning
AIMX--: this paper study gp viewed learning algorithm
AIMX--: more specifically we investigate two techniques increase generalization performance decrease complexity models use validation set select best run individuals generalize well use lexicographic parsimony pressure citation reduce complexity generated models
MISC--: techniques tested using gp encoding binary classification problems vectors taken learning sets terminals mathematical operations manipulate vectors branches
MISC--: this approach tested six different data sets uci ml repository citation
OWNX--: even if proposed techniques tested specific context we argue they extended frequent situations where gp used learning algorithm
