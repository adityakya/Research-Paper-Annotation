OWNX--: we present algorithmic framework learning multiple related tasks
OWNX--: our framework exploits form prior knowledge relates output spaces tasks
OWNX--: we present pac learning results analyze conditions under learning possible
OWNX--: we present results learning shallow parser named entity recognition system exploits our framework showing consistent improvements over baseline methods
OWNX--: when two nlp systems run same data we expect certain constraints hold between their outputs
MISC--: this form prior knowledge
OWNX--: we propose self training framework uses information significantly boost performance one systems
MISC--: key idea perform self training only outputs obey constraints
OWNX--: our motivating example this paper task pair named entity recognition ner shallow parsing aka syntactic chunking
MISC--: consider hidden sentence known pos syntactic structure below
OWNX--: further consider four potential ner sequences this sentence end small without ever seeing actual sentence we guess ner sequence correct
MISC--: ner seems wrong because we feel like named entities should not part verb phrases
OWNX--: ner seems wrong because there nnp we also include mytag nnps proper noun not part named entity word
OWNX--: ner amiss because we feel unlikely single name should span more than one np last two words
MISC--: ner none problems seems quite reasonable
MISC--: fact hidden sentence ner correct
AIMX--: remainder this paper deals problem formulating prior knowledge into workable system
OWNX--: there similarities between our proposed model both self training co training background given section
OWNX--: we present formal model our approach perform simple yet informative analysis section
OWNX--: this analysis allows us define what good bad constraints
OWNX--: throughout we use running example ner using hidden markov models show efficacy method relationship between theory implementation
OWNX--: finally we present full blown results seven different ner data sets one conll six ace comparing our method several competitive baselines section
MISC--: we see many data sets less than one hundred labeled ner sentences required get state art performance using discriminative sequence labeling algorithm citation
