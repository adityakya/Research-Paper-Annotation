OWNX--: letor website contains three information retrieval datasets used benchmark testing machine learning ideas ranking
MISC--: algorithms participating challenge required assign score values search results collection queries measured using standard ir ranking measures ndcg precision map depend only relative score induced order results
MISC--: similarly many ideas proposed participating algorithms we train linear classifier
OWNX--: contrast other participating algorithms we define additional free variable intercept benchmark each query
OWNX--: this allows expressing fact results different queries incomparable purpose determining relevance
MISC--: cost this idea addition relatively few nuisance parameters
OWNX--: our approach simple we used standard logistic regression library test
MISC--: results beat reported participating algorithms
CONT--: hence seems promising combine our approach other more complex ideas
MISC--: letor benchmark dataset citation http research
MISC--: microsoft
OWNX--: com users letor version contains three information retrieval datasets used benchmark testing machine learning ideas ranking
MISC--: algorithms participating challenge required assign score values search results collection queries measured using standard ir ranking measures ndcg symbol precision symbol map see citation details designed way only relative order results matters
MISC--: input learning problem list query result records where each record vector standard ir features together relevance label query id
MISC--: label either binary irrelevant relevant trinary irrelevant relevant very relevant
OWNX--: all reported algorithms used this task letor website citation rely fact records corresponding same query id some sense comparable each other cross query records incomparable
MISC--: rationale ir measures computed sum over queries where each query nonlinear function computed
MISC--: example ranksvm citation rankboost citation use pairs results same query penalize cost function but never cross query pairs results
OWNX--: following approach seems at first too naive compared others since training information given relevance labels why not simply train linear classifier predict relevance labels use prediction confidence score
OWNX--: unfortunately this approach fares poorly
MISC--: hypothesized reason judges relevance response may depend query
OWNX--: check this hypothesis we define additional free variable intercept benchmark each query
OWNX--: this allows expressing fact results different queries incomparable purpose determining relevance
MISC--: cost this idea addition relatively few nuisance parameters
OWNX--: our approach extremely simple we used standard logistic regression library test data
CONT--: this work not first suggest query dependent ranking but arguably simplest most immediate way address this dependence using linear classification before other complicated ideas should tested
OWNX--: based our judgment other reported algorithms used challenge more complicated our solution overall better given data
