OWNX--: we learn multiple hypotheses related tasks under latent hierarchical relationship between tasks
AIMX--: we exploit intuition domain adaptation we wish share classifier structure but multitask learning we wish share covariance structure
BASE--: our hierarchical model seen subsume several previously proposed multitask learning models performs well three distinct real world data sets
MISC--: we consider two related but distinct tasks domain adaptation da citation multitask learning mtl citation
MISC--: both involve learning related hypotheses multiple data sets
OWNX--: da we learn multiple classifiers solving same problem over data different distributions
OWNX--: mtl we learn multiple classifiers solving different problems over data same distribution
BASE--: seen bayesian perspective natural solution hierarchical model hypotheses leaves citation
MISC--: however when there more than two hypotheses learned i e more than two domains more than two tasks immediate question all hypotheses equally related
MISC--: if not what their relationship
OWNX--: we address issues proposing two hierarchical models latent hierarchies one da one mtl models nearly identical
OWNX--: we treat hierarchy nonparametrically employing kingman s coalescent citation
CONT--: we derive em algorithm makes use recently developed efficient inference algorithms coalescent citation
OWNX--: several da mtl problems we show efficacy our model
MISC--: our models da mtl share common structure based unknown hierarchy
MISC--: key difference between da model mtl model what information shared across hierarchy
OWNX--: simplicity we consider case linear classifiers logistic regression linear regression
MISC--: this extended non linear classifiers moving gaussian processes citation
MISC--: domain adaption useful model assume there single classifier does well all domains citation
OWNX--: context hierarchical bayesian modeling we interpret this saying weight vector associated linear classifier generated according hierarchical structure
MISC--: other hand mtl one does not expect same weight vector do well all problems
MISC--: instead common assumption features co vary similar ways between tasks citation
AIMX--: hierarchical bayesian model we interpret this saying covariance structure associated linear classifiers generated according hierarchical structure
OWNX--: brief da we share weights mtl we share covariance
