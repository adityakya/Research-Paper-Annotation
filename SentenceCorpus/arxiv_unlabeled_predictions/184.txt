OWNX--: we consider problem joint universal variable rate lossy coding identification parametric classes stationary symbol mixing sources general polish alphabets
MISC--: compression performance measured terms lagrangians while identification performance measured variational distance between true source estimated source
MISC--: provided sources mixing at sufficiently fast rate satisfy certain smoothness vapnik chervonenkis learnability conditions shown bounded metric distortions there exist universal schemes joint lossy compression identification whose lagrangian redundancies converge zero symbol block length symbol tends infinity where symbol vapnik chervonenkis dimension certain class decision regions defined symbol dimensional marginal distributions sources furthermore each symbol decoder identify symbol dimensional marginal active source up ball radius symbol variational distance eventually probability one
OWNX--: results supplemented several examples parametric sources satisfying regularity conditions index terms learning minimum distance density estimation two stage codes universal vector quantization vapnik chervonenkis dimension
MISC--: well known lossless source coding statistical modeling complementary objectives
MISC--: this fact captured kraft inequality see section cover thomas citation provides correspondence between uniquely decodable codes probability distributions discrete alphabet
MISC--: if one full knowledge source statistics then one design optimal lossless code source vice versa
MISC--: however practice unreasonable expect source statistics known precisely so one design universal schemes perform asymptotically optimally within given class sources
MISC--: universal coding too rissanen shown citation coding modeling objectives accomplished jointly given sufficiently regular parametric family discrete alphabet sources encoder acquire source statistics via maximum likelihood estimation sufficiently long data sequence use this knowledge select appropriate coding scheme
MISC--: even nonparametric settings e g class all stationary ergodic discrete alphabet sources universal schemes ziv lempel citation amount constructing probabilistic model source
MISC--: reverse direction kieffer citation merhav citation among others addressed problem statistical modeling parameter estimation model identification via universal lossless coding
OWNX--: once we consider lossy coding though relationship between coding modeling no longer so simple
MISC--: one hand having full knowledge source statistics certainly helpful designing optimal rate distortion codebooks
MISC--: other hand apart some special cases e g iid
MISC--: bernoulli sources hamming distortion measure iid
MISC--: gaussian sources squared error distortion measure not at all clear how extract reliable statistical model source its reproduction via rate distortion code although shown recently weissman ordentlich citation joint empirical distribution source realization corresponding codeword good rate distortion code converges distribution solving rate distortion problem source
MISC--: this not problem when emphasis compression but there situations one would like compress source identify its statistics at same time
MISC--: instance indirect adaptive control see eg chapter tao citation parameters plant controlled system estimated basis observation controller modified accordingly
MISC--: consider discrete time stochastic setting plant state sequence random process whose statistics governed finite set parameters
MISC--: suppose controller geographically separated plant connected via noiseless digital channel whose capacity symbol bits per use
MISC--: then given time horizon symbol objective design encoder decoder controller obtain reliable estimates both plant parameters plant state sequence symbol possible outputs decoder
MISC--: state problem general terms consider information source emitting sequence symbol random variables taking values alphabet symbol
MISC--: suppose process distribution symbol not specified completely but known member some parametric class symbol
AIMX--: we wish answer following two questions class symbol universally encodable respect given single letter distortion measure symbol codes given structure e g all fixed rate block codes given per letter rate all variable rate block codes etc
MISC--: other words does there exist scheme asymptotically optimal each symbol symbol
MISC--: if answer question positive codes constructed way decoder not only reconstruct source but also identify its process distribution symbol asymptotically optimal fashion
AIMX--: previous work citation we addressed two questions context fixed rate lossy block coding stationary memoryless iid continuous alphabet sources parameter space symbol bounded subset symbol some finite symbol
OWNX--: we shown under appropriate regularity conditions distortion measure source models there exist joint universal schemes lossy coding source identification whose redundancies gap between actual performance theoretical optimum given shannon distortion rate function source estimation fidelity both converge zero symbol block length symbol tends infinity
MISC--: code operates coding each block code matched source parameters estimated preceding block
MISC--: comparing this convergence rate symbol convergence rate optimal redundancies fixed rate lossy block codes citation we see there general price paid doing compression identification simultaneously
MISC--: furthermore constant hidden symbol notation increases richness model class symbol measured vapnik chervonenkis vc dimension citation certain class measurable subsets source alphabet associated sources
OWNX--: main limitation results citation iid
MISC--: assumption rather restrictive excludes many practically relevant model classes e g autoregressive sources markov hidden markov processes
OWNX--: furthermore assumption parameter space symbol bounded may not always hold at least sense we may not know diameter symbol priori
AIMX--: this paper we relax both assumptions study existence performance universal schemes joint lossy coding identification stationary sources satisfying mixing condition when sources assumed belong parametric model class symbol symbol being open subset symbol some finite symbol
OWNX--: because parameter space not bounded we use variable rate codes countably infinite codebooks performance code assessed composite lagrangian functional citation captures trade off between expected distortion expected rate code
MISC--: our result under certain regularity conditions distortion measure model class there exist universal schemes joint lossy source coding identification block length symbol tends infinity gap between actual lagrangian performance optimal lagrangian performance achievable variable rate codes at block length well source estimation fidelity at decoder converge zero symbol where symbol vc dimension certain class decision regions induced collection symbol symbol dimensional marginals source process distributions
MISC--: this result shows very clearly price paid universality terms both compression identification grows richness underlying model class captured vc dimension sequence symbol
AIMX--: richer model class harder learn affects compression performance our scheme because we use source parameters learned past data decide how encode current block
OWNX--: furthermore comparing rate at lagrangian redundancy decays zero under our scheme symbol result chou effros gray citation whose universal scheme not aimed at identification we immediately see ensuring satisfy twin objectives compression modeling we inevitably sacrifice some compression performance
AIMX--: paper organized follows
OWNX--: section introduces notation basic concepts related sources codes vapnik chervonenkis classes
OWNX--: section lists discusses regularity conditions satisfied source model class contains statement our result
OWNX--: result proved section
OWNX--: next section we give three examples parametric source families namely iid
OWNX--: gaussian sources gaussian autoregressive sources hidden markov processes fit framework this paper under suitable regularity conditions
OWNX--: we conclude section outline directions future research
MISC--: finally appendix contains some technical results lagrange optimal variable rate quantizers
