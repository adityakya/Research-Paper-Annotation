MISC--: learning appropriate distance metrics critical problem image classification retrieval
AIMX--: this work we propose boosting based technique termed boostmetric learning mahalanobis distance metric
OWNX--: one primary difficulties learning metric ensure mahalanobis matrix remains positive semidefinite
OWNX--: semidefinite programming sometimes used enforce this constraint but does not scale well
OWNX--: instead based key observation any positive semidefinite matrix decomposed into linear positive combination trace one rank one matrices
OWNX--: thus uses rank one positive semidefinite matrices weak learners within efficient scalable boosting based learning process
OWNX--: resulting method easy implement does not require tuning accommodate various types constraints
MISC--: experiments various datasets show proposed algorithm compares favorably those state art methods terms classification accuracy running time
MISC--: been extensively sought after goal learn appropriate distance metric image classification retrieval problems using simple efficient algorithms citation
MISC--: distance metrics essential effectiveness many critical algorithms symbol nearest neighbor symbol nn symbol means clustering kernel regression example
AIMX--: we show this work how mahalanobis metric learned proximity comparisons among triples training data
OWNX--: mahalanobis distance aka gaussian quadratic distance parameterized positive semidefinite psd matrix
MISC--: therefore typically methods learning mahalanobis distance result constrained semidefinite programs
OWNX--: we discuss problem setting well difficulties learning matrix
OWNX--: if we let symbol represent set points symbol training data consist set constraints upon relative distances between points symbol where symbol measures distance between symbol symbol
OWNX--: we interested case symbol computes mahalanobis distance
MISC--: mahalanobis distance between two vectors takes form symbol symbol matrix
MISC--: equivalent learn projection matrix symbol symbol
MISC--: constraints those above often arise when known symbol symbol belong same class data points while symbol belong different classes
MISC--: some cases comparison constraints much easier obtain than either class labels distances between data elements
OWNX--: example video content retrieval faces extracted successive frames at close locations safely assumed belong same person without requiring individual identified
MISC--: web search results returned search engine ranked according relevance ordering allows natural conversion into set constraints
MISC--: requirement symbol being led development number methods learning mahalanobis distance rely upon constrained semidefinite programing
OWNX--: this approach number limitations however we now discuss reference problem learning matrix set constraints upon pairwise distance comparisons
MISC--: relevant work this topic includes citation amongst others
MISC--: xing citation firstly proposed learn mahalanobis metric clustering using convex optimization
MISC--: inputs two sets similarity set dis similarity set
MISC--: algorithm maximizes distance between points dis similarity set under constraint distance between points similarity set upper bounded
MISC--: neighborhood component analysis nca citation large margin nearest neighbor lmnn citation learn metric maintaining consistency data s neighborhood keeping large margin at boundaries different classes
MISC--: been shown citation lmnn delivers state art performance among most distance metric learning algorithms
BASE--: work lmnn citation psdboost citation directly inspired our work
OWNX--: instead using hinge loss lmnn psdboost we use exponential loss function order derive adaboost like optimization procedure
OWNX--: hence despite similar purposes our algorithm differs essentially optimization
OWNX--: while formulation lmnn looks more similar support vector machines svm s psdboost lpboost our algorithm termed boostmetric largely draws upon adaboost citation
MISC--: many cases difficult find global optimum projection matrix symbol citation
MISC--: reformulation linearization typical technique convex optimization relax convexify problem citation
MISC--: metric learning much existing work instead learns symbol seeking global optimum eg citation
MISC--: price heavy computation poor scalability not trivial preserve semidefiniteness symbol during course learning
MISC--: standard approaches like interior point newton methods require hessian usually requires symbol resources where symbol input dimension
MISC--: could prohibitive many real world problems
MISC--: alternative projected sub gradient adopted citation
CONT--: disadvantages this algorithm not easy implement many parameters involved slow convergence
MISC--: psdboost citation converts particular semidefinite program metric learning into sequence linear programs lp s
MISC--: at each iteration psdboost lp needs solved lpboost scales around symbol symbol number iterations therefore variables
MISC--: symbol increases scale lp becomes larger
OWNX--: another problem psdboost needs store all weak learners rank one matrices during optimization
MISC--: when input dimension symbol large memory required proportional symbol prohibitively huge at late iteration symbol
OWNX--: our proposed algorithm solves both problems
OWNX--: based observation citation any positive semidefinite matrix decomposed into linear positive combination trace one rank one matrices we propose learning matrix
OWNX--: weak learner rank one matrix psdboost
OWNX--: proposed algorithm following desirable properties efficient scalable
MISC--: unlike most existing methods no semidefinite programming required
MISC--: at each iteration only largest eigenvalue its corresponding eigenvector needed accommodate various types constraints
OWNX--: we demonstrate learning mahalanobis metric proximity comparison constraints like adaboost does not any parameter tune
MISC--: user only needs know when stop
MISC--: contrast both lmnn psdboost parameters cross validate
MISC--: also like adaboost easy implement
OWNX--: no sophisticated optimization techniques lp solvers involved
OWNX--: unlike psdboost we do not need store all weak learners
OWNX--: efficacy efficiency proposed demonstrated various datasets
OWNX--: throughout this paper matrix denoted bold upper case letter symbol column vector denoted bold lower case letter symbol
MISC--: symbol th row symbol denoted symbol symbol th column symbol
MISC--: symbol trace symmetric matrix symbol calculates inner product two matrices
MISC--: element wise inequality between two vectors like symbol means symbol all symbol
OWNX--: we use symbol indicate matrix symbol positive semidefinite
OWNX--: matrix symbol following statements equivalent symbol symbol all eigenvalues symbol nonnegative symbol symbol symbol symbol
