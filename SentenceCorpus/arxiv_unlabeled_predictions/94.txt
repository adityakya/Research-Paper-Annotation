MISC--: higher order tensor decompositions analogous familiar singular value decomposition svd but they transcend limitations matrices second order tensors
MISC--: svd powerful tool achieved impressive results information retrieval collaborative filtering computational linguistics computational vision other fields
CONT--: however svd limited two dimensional arrays data two modes many potential applications three more modes require higher order tensor decompositions
CONT--: this paper evaluates four algorithms higher order tensor decomposition higher order singular value decomposition ho nobreakdash svd higher order orthogonal iteration hooi slice projection sp multislice projection mp
OWNX--: we measure time elapsed run time space ram disk space requirements fit tensor reconstruction accuracy four algorithms under variety conditions
OWNX--: we find standard implementations ho nobreakdash svd hooi do not scale up larger tensors due increasing ram requirements
OWNX--: we recommend hooi tensors small enough available ram mp larger tensors
MISC--: singular value decomposition svd growing increasingly popular tool analysis two dimensional arrays data due its success wide variety applications information retrieval citation collaborative filtering citation computational linguistics citation computational vision citation genomics citation
CONT--: svd limited two dimensional arrays matrices second order tensors but many applications require higher dimensional arrays known higher order tensors
MISC--: there several higher order tensor decompositions analogous svd able capture higher order structure cannot modeled two dimensions two modes
MISC--: higher order generalizations svd include higher order singular value decomposition ho nobreakdash svd citation tucker decomposition citation parafac parallel factor analysis citation also known candecomp canonical decomposition citation
MISC--: higher order tensors quickly become unwieldy
MISC--: number elements matrix increases quadratically product number rows columns but number elements third order tensor increases cubically product number rows columns tubes
MISC--: thus there need tensor decomposition algorithms handle large tensors
BASE--: this paper we evaluate four algorithms higher order tensor decomposition higher order singular value decomposition ho nobreakdash svd citation higher order orthogonal iteration hooi citation slice projection sp citation multislice projection mp introduced here
OWNX--: our main concern ability four algorithms scale up large tensors
OWNX--: section we motivate this work listing some applications higher order tensors
CONT--: any field where svd been useful there likely third fourth mode been ignored because svd only handles two modes
OWNX--: tensor notation we use this paper presented section
OWNX--: we follow notational conventions newcite kolda moh
OWNX--: section presents four algorithms ho nobreakdash svd hooi sp mp
OWNX--: ho nobreakdash svd hooi we used implementations given matlab tensor toolbox citation
OWNX--: sp mp we created our own matlab implementations
OWNX--: our implementation mp third order tensors given appendix
OWNX--: section presents our empirical evaluation four tensor decomposition algorithms
OWNX--: experiments we measure time elapsed run time space ram disk space requirements fit tensor reconstruction accuracy four algorithms under variety conditions
OWNX--: first group experiments looks at how algorithms scale input tensors grow increasingly larger
OWNX--: we test algorithms random sparse third order tensors input
MISC--: ho nobreakdash svd hooi exceed available ram when given larger tensors input but sp mp able process large tensors low ram usage good speed
MISC--: hooi provides best fit followed mp then sp lastly ho nobreakdash svd
OWNX--: second group experiments examines sensitivity fit balance ratios core sizes defined section
MISC--: algorithms tested random sparse third order tensors input
OWNX--: general fit four algorithms follows same pattern first group experiments hooi gives best fit then mp sp ho nobreakdash svd but we observe sp particularly sensitive unbalanced ratios core sizes
MISC--: third group explores fit varying ratios between size input tensor size core tensor
OWNX--: this group we move third order tensors fourth order tensors
MISC--: algorithms tested random fourth order tensors input tensor size fixed while core sizes vary
MISC--: fit algorithms follows same pattern previous two groups experiments spite move fourth order tensors
MISC--: final group measures performance real nonrandom tensor was generated task computational linguistics
MISC--: fit follows same pattern previous three groups experiments
OWNX--: furthermore differences fit reflected performance given task
CONT--: this experiment validates use random tensors previous three groups experiments
OWNX--: we conclude section
MISC--: there tradeoffs time space fit four algorithms there no absolute winner among four algorithms
MISC--: choice will depend time space fit requirements given application
OWNX--: if good fit primary concern we recommend hooi smaller tensors fit available ram mp larger tensors
