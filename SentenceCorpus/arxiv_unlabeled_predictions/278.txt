MISC--: conditional random fields crfs constitute popular efficient approach supervised sequence labelling
MISC--: crfs cope large description spaces integrate some form structural dependency between labels
OWNX--: this contribution we address issue efficient feature selection crfs based imposing sparsity through symbol penalty
OWNX--: we first show how sparsity parameter set exploited significantly speed up training labelling
OWNX--: we then introduce coordinate descent parameter update schemes crfs symbol regularization
OWNX--: we finally provide some empirical comparisons proposed approach state art crf training strategies
MISC--: particular shown proposed approach able take profit sparsity speed up processing handle larger dimensional models
MISC--: conditional random fields crfs originally introduced citation constitute popular effective approach supervised structure learning tasks involving mapping between complex objects strings trees
MISC--: important property crfs their ability cope large redundant feature sets integrate some form structural dependency between output labels
MISC--: directly modeling conditional probability label sequence given observation stream allows explicitly integrate complex dependencies not directly accounted generative models hidden markov models hmms
OWNX--: results presented section will illustrate this ability use large sets redundant non causal features
MISC--: training crf amounts solving convex optimization problem maximization penalized conditional log likelihood function
OWNX--: lack analytical solution however crf training task requires numerical optimization implies repeatedly perform inference over entire training set during computation gradient objective function
MISC--: this where modeling structure takes its toll general dependencies exact inference intractable approximations considered
MISC--: simpler case linear chain crfs modeling interaction between pairs adjacent labels makes complexity inference grow quadratically size label set even this restricted setting training crf remains computational burden especially when number output labels large
MISC--: introducing structure another less studied impact number potential features considered
OWNX--: possible linear chain crf introduce features simultaneously test values adjacent labels some property observation
MISC--: fact features often contain valuable information citation
MISC--: however their number scales quadratically number labels yielding both computational feature functions computed parameter vectors stored memory estimation problem
MISC--: estimation problem stems need estimate large parameter vectors based sparse training data
MISC--: penalizing objective function symbol norm parameter vector effective remedy overfitting yet does not decrease number feature computations needed
AIMX--: this paper we consider use alternative penalty function symbol norm yields much sparser parameter vectors citation
OWNX--: we will show inducing sparse vector not only reduces number feature functions need computed but also reduce time needed perform parameter estimation decoding
OWNX--: main shortcoming symbol regularizer objective function no longer differentiable everywhere challenging use gradient based optimization algorithms
CONT--: proposals been made overcome this difficulty instance orthant wise limited memory quasi newton algorithm citation uses fact symbol norm remains differentiable when restricted regions sign each coordinate fixed orthant
MISC--: using this technique citation reports test performance par those obtained symbol penalty albeit more compact models
OWNX--: our first contribution show even this situation equivalent test performance symbol regularization may preferable sparsity parameter set exploited reduce computational cost associated parameter training label inference
MISC--: parameter estimation we consider alternative optimization approach generalizes crfs proposal citation see also citation
MISC--: nutshell optimization performed coordinate wise fashion based analytic solution unidimensional optimization problem
OWNX--: order tackle realistic problems we propose efficient blocking scheme coordinate wise updates applied simultaneously properly selected group block parameters
OWNX--: our main methodological contributions thus twofold i fast implementation training decoding algorithms uses sparsity parameter vectors ii novel optimization algorithm using symbol penalty crfs
MISC--: two ideas combined together offer opportunity using very large virtual feature sets only very small number features effectively selected
OWNX--: will seen section this situation frequent typical natural language processing applications particularly when number possible labels large
OWNX--: finally proposed algorithm been implemented c code validated through experiments artificial real world data
MISC--: particular we provide detailed comparisons terms numerical efficiency solutions traditionally used symbol symbol penalized training crfs publicly available software crf citation crfsuite citation crfsgd citation
AIMX--: rest this paper organized follows
OWNX--: section we introduce our notations restate more precisely issues we wish address based example simple natural language processing task
OWNX--: section discusses algorithmic gains achievable when working sparse parameter vectors
OWNX--: we then study section training algorithm used achieve sparsity implements coordinate wise descent procedure
OWNX--: section discusses our contributions respect related work
OWNX--: finally section presents our experimental results obtained both simulated data phonetization task named entity recognition problem
