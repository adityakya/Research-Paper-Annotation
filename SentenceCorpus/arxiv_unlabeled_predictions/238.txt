AIMX--: we present novel approach learning nonlinear dynamic models leads new set tools capable solving problems otherwise difficult
OWNX--: we provide theory showing this new approach consistent models long range structure apply approach motion capture high dimensional video data yielding results superior standard alternatives
MISC--: notion hidden states appears many nonstationary models world hidden markov models hmms discrete states kalman filters continuous states
MISC--: figure shows general dynamic model observation symbol unobserved hidden state symbol
MISC--: system characterized state transition probability symbol state observation probability symbol
OWNX--: method predicting future events under dynamic model maintain posterior distribution over hidden state symbol based all observations symbol up time symbol
MISC--: posterior updated using formula prediction future events symbol symbol conditioned symbol through posterior over symbol hidden state based dynamic models wide range applications time series forecasting finance control robotics video speech processing
MISC--: some detailed dynamic models application examples found citation
MISC--: eqref eq predict clear benefit using hidden state dynamic model information contained observation symbol captured relatively small hidden state symbol
CONT--: therefore order predict future we do not use all previous observations symbol but only its state representation symbol
MISC--: principle symbol may contain finite history length symbol symbol
CONT--: although notation only considers first order dependency incorporates higher order dependency considering representation form symbol standard trick hmm kalman filter both transition observation functions linear maps
MISC--: there reasonable algorithms learn linear dynamic models
MISC--: example addition classical em approach was recently shown global learning certain hidden markov models achieved polynomial time citation
MISC--: moreover linear models posterior update rule quite simple
MISC--: therefore once model parameters estimated models readily applied prediction
MISC--: however many real problems system dynamics cannot approximated linearly
MISC--: problems often necessary incorporate nonlinearity into dynamic model
MISC--: standard approach this problem through nonlinear probability modeling where prior knowledge required define sensible state representation together parametric forms transition observation probabilities
MISC--: model parameters learned using probabilistic methods em citation
MISC--: when learned model applied prediction purposes necessary maintain posterior symbol using update formula eqref eq post update
MISC--: unfortunately nonlinear systems maintaining symbol generally difficult because posterior become exponentially more complex e g exponentially many mixture components mixture model symbol increases
MISC--: this computational difficulty significant obstacle applying nonlinear dynamic systems practical problems
MISC--: traditional approach address computational difficulty through approximation methods
MISC--: example particle filtering approach citation one uses finite number samples represent posterior distribution samples then updated observations arrive
MISC--: another approach maintain mixture gaussians approximate posterior symbol may also regarded mixture kalman filters citation
BASE--: although exponential symbol number mixture components needed accurately represent posterior practice one use fixed number mixture components approximate distribution
MISC--: this leads following question even if posterior well approximated computationally tractable approximation family finite mixtures gaussians how one design good approximate inference method guaranteed find good quality approximation
MISC--: use complex techniques required design reasonable approximation schemes makes non trivial apply nonlinear dynamic models many practical problems
AIMX--: this paper introduces alternative approach where we start different representation linear dynamic model we call sufficient posterior representation
MISC--: shown one recover underlying state representation using prediction methods not necessarily probabilistic
OWNX--: this allows us model nonlinear dynamic behaviors many available nonlinear supervised learning algorithms neural networks boosting support vector machines simple unified fashion
CONT--: compared traditional approach several distinct advantages does not require us design any explicit state representation probability model using prior knowledge
MISC--: instead representation implicitly embedded representational choice underlying supervised learning algorithm may regarded black box power learn arbitrary representation
MISC--: prior knowledge simply encoded input features learning algorithms significantly simplifies modeling aspect
MISC--: does not require us come up any specific representation posterior corresponding approximate bayesian inference schemes posterior updates
MISC--: instead this issue addressed incorporating posterior update part learning process
MISC--: again posterior representation implicitly embedded representational choice underlying supervised learning algorithm
OWNX--: this sense our scheme learns optimal representation posterior approximation corresponding update rules within representational power underlying supervised algorithm
CONT--: possible obtain performance guarantees our algorithm terms learning performance underlying supervised algorithm
MISC--: performance latter been heavily investigated statistical learning theory literature
OWNX--: results thus applied obtain theoretical results our methods learning nonlinear dynamic models
