MISC--: problem joint universal source coding modeling treated context lossless codes rissanen was recently generalized fixed rate lossy coding finitely parametrized continuous alphabet iid
MISC--: sources
OWNX--: we extend results variable rate lossy block coding stationary ergodic sources show bounded metric distortion measures any finitely parametrized family stationary sources satisfying suitable mixing smoothness vapnik chervonenkis learnability conditions admits universal schemes joint lossy source coding identification
OWNX--: we also give several explicit examples parametric sources satisfying regularity conditions
MISC--: universal source coding scheme one performs asymptotically optimally all sources within given class
MISC--: intuition suggests good universal coder should acquire probabilistic model source sufficiently long data sequence operate based this model
MISC--: lossless codes this intuition been made rigorous rissanen citation data encoded via two part code comprises suitably quantized maximum likelihood estimate source parameters encoding data code optimized acquired model
MISC--: redundancy this scheme converges zero symbol where symbol block length symbol dimension parameter space
OWNX--: recently we extended rissanen s ideas lossy block coding finitely parametrized continuous alphabet iid
MISC--: sources bounded parameter spaces citation
OWNX--: we shown under appropriate regularity conditions there exist joint universal schemes lossy coding source identification whose distortion redundancy source estimation fidelity both converge zero symbol block length symbol tends infinity
MISC--: code operates coding each block code matched parameters estimated preceding block
MISC--: moreover constant hidden symbol notation increases richness model class measured vapnik chervonenkis vc dimension citation certain class decision regions source alphabet
OWNX--: main limitation results citation iid
MISC--: assumption excludes practically relevant model classes autoregressive sources markov hidden markov processes
MISC--: furthermore assumption bounded parameter space may not always justified
AIMX--: this paper we relax both assumptions
BASE--: because parameter space not bounded we use variable rate codes countably infinite codebooks whose performance naturally quantified lagrangians citation
MISC--: we show under certain regularity conditions there universal schemes joint lossy source coding modeling block length symbol tends infinity both lagrangian redundancy relative best variable rate code at each block length source estimation fidelity at decoder converge zero symbol where symbol vc dimension certain class decision regions induced collection all symbol dimensional marginals source process distributions
OWNX--: key novel feature our scheme unlike most existing schemes universal lossy coding rely implicit identification active source learns explicit probabilistic model
OWNX--: moreover our results clearly show price universality modeling based compression scheme grows combinatorial richness underlying model class captured vc dimension sequence symbol
AIMX--: richer model class harder learn turn affects compression performance because we use source parameters learned past data deciding how encode current block
MISC--: insights may prove useful settings digital forensics adaptive control under communication constraints where trade offs between quality parameter estimation compression performance central importance
