AIMX--: this paper we propose algorithm polynomial time reinforcement learning factored markov decision processes fmdps
CONT--: factored optimistic initial model foim algorithm maintains empirical model fmdp conventional way always follows greedy policy respect its model
CONT--: only trick algorithm model initialized optimistically
OWNX--: we prove suitable initialization i foim converges fixed point approximate value iteration avi ii number steps when agent makes non near optimal decisions respect solution avi polynomial all relevant quantities iii per step costs algorithm also polynomial
OWNX--: our best knowledge foim first algorithm properties
OWNX--: this extended version contains rigorous proofs main theorem
AIMX--: version this paper appeared icml
OWNX--: factored markov decision processes fmdps practical ways compactly formulate sequential decision problems provided we ways solve them
MISC--: when environment unknown all effective reinforcement learning methods apply some form optimism face uncertainty principle whenever learning agent faces unknown should assume high rewards order encourage exploration
MISC--: factored optimistic initial model foim takes this principle extreme its model initialized overly optimistic
MISC--: more often visited areas state space model gradually gets more realistic inspiring agent head unknown regions explore them search some imaginary garden eden
CONT--: working algorithm simple extreme will not make any explicit effort balance exploration exploitation but always follows greedy optimal policy respect its model
AIMX--: we show this paper this simple even simplistic trick sufficient effective fmdp learning
CONT--: algorithm extension oim optimistic initial model citation sample efficient learning algorithm flat mdps
MISC--: there important difference however way model solved
MISC--: every time model updated corresponding value function needs re calculated updated flat mdps this not problem various dynamic programming based algorithms like value iteration solve model any required accuracy polynomial time
MISC--: situation less bright generating near optimal fmdp solutions all currently known algorithms may take exponential time eg approximate policy iteration citation using decision tree representations policies solving exponential size flattened version fmdp
AIMX--: if we require polynomial running time we do this paper search practical algorithm then we accept sub optimal solutions
MISC--: only known example polynomial time fmdp planner factored value iteration fvi citation will serve base planner our learning method
OWNX--: this planner guaranteed converge error its solution bounded term depending only quality function approximators
MISC--: our analysis algorithm will follow established techniques analyzing sample efficient reinforcement learning like works citation flat mdps citation fmdps
MISC--: however listed proofs convergence rely critically access near optimal planner so they generalized suitably
OWNX--: doing so we able show foim converges bounded error solution polynomial time high probability
OWNX--: we introduce basic concepts notations section then section we review existing work special emphasis immediate ancestors our method
OWNX--: sections we describe blocks foim foim algorithm respectively
AIMX--: we finish paper short analysis discussion
