OWNX--: we define novel basic unsupervised learning problem learning lowest density homogeneous hyperplane separator unknown probability distribution
MISC--: this task relevant several problems machine learning semi supervised learning clustering stability
OWNX--: we investigate question existence universally consistent algorithm this problem
OWNX--: we propose two natural learning paradigms prove input unlabeled random samples generated any member rich family distributions they guaranteed converge optimal separator distribution
OWNX--: we complement this result showing no learning algorithm our task achieve uniform learning rates independent data generating distribution
MISC--: while theory machine learning achieved extensive understanding many aspects supervised learning our theoretical understanding unsupervised learning leaves lot desired
OWNX--: spite obvious practical importance various unsupervised learning tasks state our current knowledge does not provide anything comes close rigorous mathematical performance guarantees classification prediction theory enjoys
OWNX--: this paper we make small step direction analyzing one specific unsupervised learning task detection low density linear separators data distributions over euclidean spaces
OWNX--: we consider following task unknown data distribution over symbol find homogeneous hyperplane lowest density cuts through distribution
OWNX--: we assume underlying data distribution continuous density function data available learner finite iid
MISC--: samples distribution
OWNX--: our model viewed restricted instance fundamental issue inferring information about probability distribution random samples generates
MISC--: tasks nature range ambitious problem density estimation citation through estimation level sets citation citation citation densest region detection citation course clustering
MISC--: all tasks notoriously difficult respect both sample complexity computational complexity aspects unless one presumes strong restrictions about nature underlying data distribution
OWNX--: our task seems more modest than
BASE--: although we not aware any previous work this problem point view statistical machine learning at least we believe rather basic problem relevant various practical learning scenarios
MISC--: one important domain detection low density linear data separators relevant semi supervised learning citation
MISC--: semi supervised learning motivated fact many real world classification problems unlabeled samples much cheaper easier obtain than labeled examples
MISC--: consequently there great incentive develop tools unlabeled samples utilized improve quality sample based classifiers
MISC--: naturally utility unlabeled data classification depends assuming some relationship between unlabeled data distribution class membership data points see citation rigorous discussion this point
OWNX--: common postulate type boundary between data classes passes through low density regions data distribution
OWNX--: transductive support vector machines paradigm tsvm citation example algorithm implicitly uses low density boundary assumption
OWNX--: roughly speaking tsvm searches hyperplane small error labeled data at same time wide margin respect unlabeled data sample
MISC--: another area low density boundaries play significant role analysis clustering stability
MISC--: recent work analysis clustering stability found close relationship between stability clustering data density along cluster boundaries roughly speaking lower densities more stable clustering citation citation
OWNX--: low density cut algorithm family symbol probability distributions takes input finite sample generated some distribution symbol output hyperplane through origin low density w r t
MISC--: symbol
OWNX--: particular we consider family all distributions over symbol continuous density functions
OWNX--: we investigate two notions success low density cut algorithms uniform convergence over family probability distributions consistency
MISC--: uniform convergence we prove general negative result showing no algorithm guarantee any fixed convergence rates terms sample sizes
MISC--: this negative result holds even simplest case where data domain one dimensional unit interval
MISC--: consistency e g allowing learning convergence rates depend data generating distribution we prove success two natural algorithmic paradigms soft margin algorithms choose margin parameter depending sample size output separator lowest empirical weight margins around hard margin algorithms choose separator widest sample free margins
OWNX--: paper organized follows section provides formal definition our learning task well success criteria we investigate
BASE--: section we present two natural learning paradigms problem over real line prove their universal consistency over rich class probability distributions
OWNX--: section extends results show learnability lowest density homogeneous linear cuts probability distributions over symbol arbitrary dimension symbol
OWNX--: section we show previous universal consistency results cannot improved obtain uniform learning rates any finite sample based algorithm
OWNX--: we conclude paper discussion directions further research
